{"url": "https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q6_K.gguf", "gguf_file_path": "c:\\Users\\laelal.halawani\\Desktop\\python\\glai\\glai\\back_end\\model_db\\gguf_models\\mixtral-8x7b-instruct-v0.1.Q6_K.gguf", "model_name": "mixtral-8x7b-instruct-v0.1", "model_quantization": "Q6_K", "description": "The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested.", "keywords": ["mixtral", "8x7b", "instruct", "v0.1", "MoE"], "user_tags": {"open": "[INST]", "close": "[/INST]"}, "ai_tags": {"open": "", "close": ""}, "system_tags": {"open": null, "close": null}, "save_dir": "c:\\Users\\laelal.halawani\\Desktop\\python\\glai\\glai\\back_end\\model_db\\gguf_models"}